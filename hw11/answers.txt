1. What parameters did you change? 

Base:
The inference and update process.


2. What values did you try?

Base:


Change 1:
- Adam was replaced with Adamax. Adamax is supposed to work better with sparse parameter updates. The gradient term is essentially ignored when itâ€™s small therefore parameters are less susceptible to gradient noise.


Change 2:
- Update the number of layers and nodes
- def nnmodel(input_dim):
    model = Sequential()
    model.add(Dense(64, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, input_dim=input_dim, activation='relu'))
    model.add(Dense(32, activation='sigmoid'))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adamax', metrics=['accuracy'])
    return model



3. Did you try any other changes that made things better or worse? Did they improve or degrade the model? 

Base:
At step  50000
reward:  -4.262057550031028
total rewards  229.75075210891367
loss: 174.4578


Change 1 - adam to adamax:
At step  50000
reward:  -7.917616013448585
total rewards  174.91816134397675
<pre>loss: 158.1465 </pre>


Change 2 - change layers:
At step  50000
reward:  -7.73850445549951
total rewards  -368.4289377267589
loss: 171.5462 


4. Based on what you observed, what conclusions can you draw about the different parameters and their values?
